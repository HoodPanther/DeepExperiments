{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks \n",
    "\n",
    "For an introduction to RNN take a look at [this great article](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "rnd.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Tensorflow \n",
    "import tensorflow as tf\n",
    "\n",
    "# \n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"b<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Manual RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.98634392  0.81788576  0.8470158  -0.99247205  0.68880105]\n",
      " [-1.         -0.95546728  0.99203932 -0.99999928 -0.45464534]\n",
      " [-1.         -0.99989605  0.99961442 -1.         -0.94950742]\n",
      " [-1.         -0.99988145 -0.99999958 -1.         -0.99962741]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})\n",
    "    \n",
    "print(Y0_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -1.          0.96730083 -1.         -0.99997342]\n",
      " [ 0.8294723   0.02448707 -0.94491893 -0.92168093 -0.9122532 ]\n",
      " [-1.         -0.99999344 -0.28294998 -0.99999934 -0.99998403]\n",
      " [-0.99977511 -0.99972141  0.11019414 -0.98003727 -0.99999589]]\n"
     ]
    }
   ],
   "source": [
    "print(Y1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using rnn()\n",
    "\n",
    "The static_rnn() function creates an unrolled RNN network by chaining cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48671275, -0.48561448,  0.86629325, -0.72946197, -0.53452528],\n",
       "       [-0.99417853, -0.77786744,  0.99997634,  0.10450334,  0.41930473],\n",
       "       [-0.99995065, -0.91371775,  1.        ,  0.8135196 ,  0.90335333],\n",
       "       [-0.96949047, -0.57477111,  0.9999451 ,  0.99996626,  0.99443597]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32)\n",
    "Y0, Y1 = output_seqs\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})\n",
    "    \n",
    "Y0_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99998569, -0.86334312,  1.        ,  0.99911886,  0.99962145],\n",
       "       [ 0.3538464 ,  0.68475449,  0.39988694,  0.07795403,  0.6127547 ],\n",
       "       [-0.999672  ,  0.78515959,  0.99999964,  0.99463904,  0.99695826],\n",
       "       [-0.96491492,  0.90873224,  0.99751562,  0.96221972,  0.96019566]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dynamic_rnn()\n",
    "\n",
    "The dynamic_rnn() function uses a while_loop() operation to run over the cell the appropriate number of times, and you can set swap_memory = True if you want it to swap the GPU’s memory to the CPU’s memory during backpropagation to avoid OOM errors. Conveniently, it also accepts a single tensor for all inputs at every time step (shape [None, n_steps, n_inputs]) and it outputs a single tensor for all outputs at every time step (shape [None, n_steps, n_neurons]); there is no need to stack, unstack, or transpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs = [[[-0.65551144  0.73566824 -0.85184079  0.1408051  -0.67453671]\n",
      "  [ 0.99524468  0.91472399 -1.          0.99938375  0.91451889]]\n",
      "\n",
      " [[-0.37057179  0.9426769  -0.99993008  0.88595641 -0.4519437 ]\n",
      "  [ 0.61579293  0.03843237  0.7278012   0.42999566  0.21875195]]\n",
      "\n",
      " [[ 0.00673045  0.98863083 -1.00000012  0.99033666 -0.15402398]\n",
      "  [ 0.95808011  0.56590128 -0.99997652  0.9931702   0.9524017 ]]\n",
      "\n",
      " [[ 0.92239261 -0.67527854 -0.99997818  0.99998701  0.99996889]\n",
      "  [ 0.14767335 -0.83654004 -0.99008358  0.92541546  0.95951402]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X_batch = np.array([\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(\"outputs =\", outputs.eval(feed_dict={X: X_batch}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.79451835 -0.44819504 -0.59462738 -0.24471386  0.91153216]\n",
      "  [-0.9990105   0.97848833 -0.99174058  0.93728083  0.99999642]]\n",
      "\n",
      " [[-0.98845619 -0.0552582  -0.95545596  0.09708088  0.99973947]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.99941158  0.35557428 -0.9959259   0.4174149   0.99999923]\n",
      "  [-0.98805785  0.81141108 -0.9052664   0.9342975   0.99974763]]\n",
      "\n",
      " [[ 0.77583563  0.58787191 -0.86811101  0.99349481  0.31281272]\n",
      "  [-0.45480198 -0.14292759 -0.28857675  0.82472932  0.97620714]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "seq_length = tf.placeholder(tf.int32, [None])   ### <----------------------------------------\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, sequence_length=seq_length, dtype=tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "\n",
    "\n",
    "seq_length_batch = np.array([2, 1, 2, 2])  ### <------------------------\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})\n",
    "    \n",
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9990105   0.97848833 -0.99174058  0.93728083  0.99999642]\n",
      " [-0.98845619 -0.0552582  -0.95545596  0.09708088  0.99973947]\n",
      " [-0.98805785  0.81141108 -0.9052664   0.9342975   0.99974763]\n",
      " [-0.45480198 -0.14292759 -0.28857675  0.82472932  0.97620714]]\n"
     ]
    }
   ],
   "source": [
    "print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a sequence classifier\n",
    "\n",
    "We will treat each image as a sequence of 28 rows of 28 pixels each (since each MNIST image is 28 × 28 pixels). We will use cells of 150 recurrent neurons, plus a fully connected layer containing 10 neurons (one per class) connected to the output of the last time step, followed by a softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.92 Test accuracy: 0.9297\n",
      "1 Train accuracy: 0.953333 Test accuracy: 0.9524\n",
      "2 Train accuracy: 0.96 Test accuracy: 0.9613\n",
      "3 Train accuracy: 0.98 Test accuracy: 0.9688\n",
      "4 Train accuracy: 0.946667 Test accuracy: 0.9607\n",
      "5 Train accuracy: 0.96 Test accuracy: 0.9676\n",
      "6 Train accuracy: 0.98 Test accuracy: 0.9729\n",
      "7 Train accuracy: 0.973333 Test accuracy: 0.9747\n",
      "8 Train accuracy: 0.973333 Test accuracy: 0.9711\n",
      "9 Train accuracy: 0.966667 Test accuracy: 0.9746\n",
      "10 Train accuracy: 0.986667 Test accuracy: 0.974\n",
      "11 Train accuracy: 0.993333 Test accuracy: 0.9731\n",
      "12 Train accuracy: 0.973333 Test accuracy: 0.972\n",
      "13 Train accuracy: 0.993333 Test accuracy: 0.968\n",
      "14 Train accuracy: 0.986667 Test accuracy: 0.9779\n",
      "15 Train accuracy: 1.0 Test accuracy: 0.9778\n",
      "16 Train accuracy: 0.993333 Test accuracy: 0.9828\n",
      "17 Train accuracy: 0.993333 Test accuracy: 0.9757\n",
      "18 Train accuracy: 1.0 Test accuracy: 0.9759\n",
      "19 Train accuracy: 0.986667 Test accuracy: 0.979\n",
      "20 Train accuracy: 0.973333 Test accuracy: 0.9747\n",
      "21 Train accuracy: 0.986667 Test accuracy: 0.9786\n",
      "22 Train accuracy: 0.98 Test accuracy: 0.9775\n",
      "23 Train accuracy: 0.986667 Test accuracy: 0.9792\n",
      "24 Train accuracy: 0.993333 Test accuracy: 0.9798\n",
      "25 Train accuracy: 0.993333 Test accuracy: 0.9741\n",
      "26 Train accuracy: 0.98 Test accuracy: 0.9793\n",
      "27 Train accuracy: 1.0 Test accuracy: 0.9755\n",
      "28 Train accuracy: 0.986667 Test accuracy: 0.9823\n",
      "29 Train accuracy: 0.993333 Test accuracy: 0.9802\n",
      "30 Train accuracy: 0.986667 Test accuracy: 0.9804\n",
      "31 Train accuracy: 0.986667 Test accuracy: 0.9781\n",
      "32 Train accuracy: 0.986667 Test accuracy: 0.9757\n",
      "33 Train accuracy: 0.993333 Test accuracy: 0.9795\n",
      "34 Train accuracy: 0.993333 Test accuracy: 0.9761\n",
      "35 Train accuracy: 0.993333 Test accuracy: 0.9778\n",
      "36 Train accuracy: 1.0 Test accuracy: 0.9843\n",
      "37 Train accuracy: 0.986667 Test accuracy: 0.9827\n",
      "38 Train accuracy: 0.993333 Test accuracy: 0.9758\n",
      "39 Train accuracy: 1.0 Test accuracy: 0.9811\n",
      "40 Train accuracy: 0.98 Test accuracy: 0.9806\n",
      "41 Train accuracy: 0.986667 Test accuracy: 0.9813\n",
      "42 Train accuracy: 0.986667 Test accuracy: 0.9828\n",
      "43 Train accuracy: 0.993333 Test accuracy: 0.978\n",
      "44 Train accuracy: 1.0 Test accuracy: 0.9844\n",
      "45 Train accuracy: 0.993333 Test accuracy: 0.9823\n",
      "46 Train accuracy: 1.0 Test accuracy: 0.9828\n",
      "47 Train accuracy: 1.0 Test accuracy: 0.9836\n",
      "48 Train accuracy: 0.986667 Test accuracy: 0.9814\n",
      "49 Train accuracy: 0.993333 Test accuracy: 0.981\n",
      "50 Train accuracy: 1.0 Test accuracy: 0.9831\n",
      "51 Train accuracy: 0.986667 Test accuracy: 0.9816\n",
      "52 Train accuracy: 0.993333 Test accuracy: 0.9839\n",
      "53 Train accuracy: 0.993333 Test accuracy: 0.9824\n",
      "54 Train accuracy: 0.993333 Test accuracy: 0.981\n",
      "55 Train accuracy: 0.993333 Test accuracy: 0.9827\n",
      "56 Train accuracy: 1.0 Test accuracy: 0.9818\n",
      "57 Train accuracy: 0.993333 Test accuracy: 0.9791\n",
      "58 Train accuracy: 0.986667 Test accuracy: 0.9825\n",
      "59 Train accuracy: 0.993333 Test accuracy: 0.9812\n",
      "60 Train accuracy: 0.993333 Test accuracy: 0.9787\n",
      "61 Train accuracy: 1.0 Test accuracy: 0.9824\n",
      "62 Train accuracy: 0.993333 Test accuracy: 0.9764\n",
      "63 Train accuracy: 0.993333 Test accuracy: 0.9836\n",
      "64 Train accuracy: 0.993333 Test accuracy: 0.9823\n",
      "65 Train accuracy: 1.0 Test accuracy: 0.9812\n",
      "66 Train accuracy: 1.0 Test accuracy: 0.9825\n",
      "67 Train accuracy: 0.98 Test accuracy: 0.9793\n",
      "68 Train accuracy: 0.986667 Test accuracy: 0.9814\n",
      "69 Train accuracy: 0.993333 Test accuracy: 0.9801\n",
      "70 Train accuracy: 0.993333 Test accuracy: 0.9804\n",
      "71 Train accuracy: 0.993333 Test accuracy: 0.9834\n",
      "72 Train accuracy: 1.0 Test accuracy: 0.9823\n",
      "73 Train accuracy: 1.0 Test accuracy: 0.983\n",
      "74 Train accuracy: 1.0 Test accuracy: 0.9833\n",
      "75 Train accuracy: 0.993333 Test accuracy: 0.9806\n",
      "76 Train accuracy: 0.966667 Test accuracy: 0.9776\n",
      "77 Train accuracy: 0.986667 Test accuracy: 0.9775\n",
      "78 Train accuracy: 0.993333 Test accuracy: 0.9851\n",
      "79 Train accuracy: 0.986667 Test accuracy: 0.9831\n",
      "80 Train accuracy: 0.993333 Test accuracy: 0.9837\n",
      "81 Train accuracy: 1.0 Test accuracy: 0.9818\n",
      "82 Train accuracy: 1.0 Test accuracy: 0.982\n",
      "83 Train accuracy: 1.0 Test accuracy: 0.9823\n",
      "84 Train accuracy: 0.986667 Test accuracy: 0.9834\n",
      "85 Train accuracy: 1.0 Test accuracy: 0.9825\n",
      "86 Train accuracy: 1.0 Test accuracy: 0.9823\n",
      "87 Train accuracy: 0.986667 Test accuracy: 0.9793\n",
      "88 Train accuracy: 0.993333 Test accuracy: 0.9824\n",
      "89 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "90 Train accuracy: 1.0 Test accuracy: 0.9799\n",
      "91 Train accuracy: 0.993333 Test accuracy: 0.9806\n",
      "92 Train accuracy: 1.0 Test accuracy: 0.9847\n",
      "93 Train accuracy: 0.993333 Test accuracy: 0.9852\n",
      "94 Train accuracy: 1.0 Test accuracy: 0.9836\n",
      "95 Train accuracy: 1.0 Test accuracy: 0.9835\n",
      "96 Train accuracy: 1.0 Test accuracy: 0.9831\n",
      "97 Train accuracy: 1.0 Test accuracy: 0.9829\n",
      "98 Train accuracy: 0.993333 Test accuracy: 0.9803\n",
      "99 Train accuracy: 1.0 Test accuracy: 0.9813\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "with tf.variable_scope(\"rnn\", initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = fully_connected(states, n_outputs, activation_fn=None)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the same sequence classifier with Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Evaluate IRNN...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.7979 - acc: 0.7312 - val_loss: 0.3889 - val_acc: 0.8750\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.3082 - acc: 0.9014 - val_loss: 0.2438 - val_acc: 0.9230\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.2165 - acc: 0.9318 - val_loss: 0.2059 - val_acc: 0.9335\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.1745 - acc: 0.9449 - val_loss: 0.1507 - val_acc: 0.9531\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.1429 - acc: 0.9544 - val_loss: 0.1242 - val_acc: 0.9608\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.1263 - acc: 0.9608 - val_loss: 0.1186 - val_acc: 0.9642\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.1089 - acc: 0.9658 - val_loss: 0.1103 - val_acc: 0.9654\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0993 - acc: 0.9686 - val_loss: 0.1116 - val_acc: 0.9652\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0955 - acc: 0.9699 - val_loss: 0.1118 - val_acc: 0.9648\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0896 - acc: 0.9726 - val_loss: 0.1025 - val_acc: 0.9686\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0772 - acc: 0.9762 - val_loss: 0.0877 - val_acc: 0.9728\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0732 - acc: 0.9770 - val_loss: 0.0735 - val_acc: 0.9749\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0703 - acc: 0.9778 - val_loss: 0.0839 - val_acc: 0.9715\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0646 - acc: 0.9795 - val_loss: 0.0946 - val_acc: 0.9717\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0571 - acc: 0.9821 - val_loss: 0.0799 - val_acc: 0.9765\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0558 - acc: 0.9824 - val_loss: 0.0701 - val_acc: 0.9782\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0508 - acc: 0.9839 - val_loss: 0.1032 - val_acc: 0.9692\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0484 - acc: 0.9847 - val_loss: 0.0682 - val_acc: 0.9790\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0463 - acc: 0.9852 - val_loss: 0.0682 - val_acc: 0.9793\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0424 - acc: 0.9869 - val_loss: 0.0745 - val_acc: 0.9781\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0415 - acc: 0.9870 - val_loss: 0.0682 - val_acc: 0.9781.\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0371 - acc: 0.9879 - val_loss: 0.0692 - val_acc: 0.9794\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0389 - acc: 0.9877 - val_loss: 0.0636 - val_acc: 0.9827\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0336 - acc: 0.9890 - val_loss: 0.0645 - val_acc: 0.9817\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0318 - acc: 0.9895 - val_loss: 0.0570 - val_acc: 0.9812\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0314 - acc: 0.9900 - val_loss: 0.0588 - val_acc: 0.9825\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0267 - acc: 0.9915 - val_loss: 0.0638 - val_acc: 0.9803\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0273 - acc: 0.9909 - val_loss: 0.0639 - val_acc: 0.9812\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0278 - acc: 0.9912 - val_loss: 0.0696 - val_acc: 0.9805\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0243 - acc: 0.9919 - val_loss: 0.0725 - val_acc: 0.9809\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0248 - acc: 0.9922 - val_loss: 0.0653 - val_acc: 0.9828\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0221 - acc: 0.9926 - val_loss: 0.0621 - val_acc: 0.9833\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0226 - acc: 0.9928 - val_loss: 0.0694 - val_acc: 0.9817\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0203 - acc: 0.9931 - val_loss: 0.0657 - val_acc: 0.9818\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0192 - acc: 0.9937 - val_loss: 0.0590 - val_acc: 0.9836\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0185 - acc: 0.9937 - val_loss: 0.0611 - val_acc: 0.9825\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0611 - val_acc: 0.9848\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0171 - acc: 0.9945 - val_loss: 0.0712 - val_acc: 0.9834\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0155 - acc: 0.9951 - val_loss: 0.0622 - val_acc: 0.9831\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0146 - acc: 0.9954 - val_loss: 0.0668 - val_acc: 0.9822\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0147 - acc: 0.9953 - val_loss: 0.0626 - val_acc: 0.9829\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0131 - acc: 0.9957 - val_loss: 0.0611 - val_acc: 0.9855\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0150 - acc: 0.9952 - val_loss: 0.0666 - val_acc: 0.9831\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0118 - acc: 0.9964 - val_loss: 0.0604 - val_acc: 0.9849\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0803 - val_acc: 0.9801\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0130 - acc: 0.9959 - val_loss: 0.0763 - val_acc: 0.9813\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0118 - acc: 0.9961 - val_loss: 0.0677 - val_acc: 0.9835\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0692 - val_acc: 0.9847\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0691 - val_acc: 0.9816\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0123 - acc: 0.9960 - val_loss: 0.0637 - val_acc: 0.9850\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0666 - val_acc: 0.9845\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0101 - acc: 0.9969 - val_loss: 0.0767 - val_acc: 0.9844\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0806 - val_acc: 0.9815\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0106 - acc: 0.9965 - val_loss: 0.0745 - val_acc: 0.9845\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0770 - val_acc: 0.9842\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0720 - val_acc: 0.9834\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0071 - acc: 0.9977 - val_loss: 0.0708 - val_acc: 0.9837\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0082 - acc: 0.9972 - val_loss: 0.0738 - val_acc: 0.9845\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0088 - acc: 0.9969 - val_loss: 0.0817 - val_acc: 0.9817\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0700 - val_acc: 0.9839\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0725 - val_acc: 0.9836\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 21s - loss: 0.0077 - acc: 0.9975 - val_loss: 0.0728 - val_acc: 0.9841\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0724 - val_acc: 0.9831\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0812 - val_acc: 0.9839\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0799 - val_acc: 0.9837\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1002 - val_acc: 0.9821\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0072 - acc: 0.9977 - val_loss: 0.0790 - val_acc: 0.9841.0073 \n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0788 - val_acc: 0.9839\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0979 - val_acc: 0.9791\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0839 - val_acc: 0.9839\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0744 - val_acc: 0.9857\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0048 - acc: 0.9983 - val_loss: 0.0688 - val_acc: 0.9858\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0058 - acc: 0.9982 - val_loss: 0.0898 - val_acc: 0.9836\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0897 - val_acc: 0.9829\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0704 - val_acc: 0.9855\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0792 - val_acc: 0.9842\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0889 - val_acc: 0.9837\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0764 - val_acc: 0.9846\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0842 - val_acc: 0.9841\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0040 - acc: 0.9990 - val_loss: 0.0765 - val_acc: 0.9861\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 19s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0842 - val_acc: 0.9837\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0840 - val_acc: 0.9853.\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0853 - val_acc: 0.9837\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0062 - acc: 0.9983 - val_loss: 0.0775 - val_acc: 0.9853\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 20s - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0793 - val_acc: 0.9852\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 19s - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0912 - val_acc: 0.9827\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0028 - acc: 0.9993 - val_loss: 0.0795 - val_acc: 0.9849\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0862 - val_acc: 0.9837\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0036 - acc: 0.9991 - val_loss: 0.0833 - val_acc: 0.9860\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0937 - val_acc: 0.9843\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0879 - val_acc: 0.9843\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0903 - val_acc: 0.9816\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0958 - val_acc: 0.9830\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0837 - val_acc: 0.9851\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0925 - val_acc: 0.9848\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0879 - val_acc: 0.9844\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 21s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0995 - val_acc: 0.9838\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0882 - val_acc: 0.9830\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 22s - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0917 - val_acc: 0.9838\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 23s - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0874 - val_acc: 0.9844\n",
      "IRNN test score: 0.0874122375119\n",
      "IRNN test accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "batch_size = 150\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "hidden_units = 150\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('Evaluate IRNN...')\n",
    "a = Input(shape=x_train.shape[1:])\n",
    "b = SimpleRNN(hidden_units,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(),\n",
    "                    activation='relu')(a)\n",
    "b = Dense(num_classes)(b)\n",
    "b = Activation('softmax')(b)\n",
    "optimizer = keras.optimizers.Adamax(lr=learning_rate)\n",
    "model = Model(inputs=[a], outputs=[b])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('IRNN test score:', scores[0])\n",
    "print('IRNN test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer RNN\n",
    "\n",
    "It is quite common to stack multiple layers of cells. This gives you a __deep RNN__.\n",
    "To implement a deep RNN in TensorFlow, you can create several cells and stack them into a __MultiRNNCell__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.926667 Test accuracy: 0.9232\n",
      "1 Train accuracy: 0.973333 Test accuracy: 0.9529\n",
      "2 Train accuracy: 0.966667 Test accuracy: 0.9612\n",
      "3 Train accuracy: 0.98 Test accuracy: 0.9734\n",
      "4 Train accuracy: 0.973333 Test accuracy: 0.968\n",
      "5 Train accuracy: 0.973333 Test accuracy: 0.9664\n",
      "6 Train accuracy: 0.98 Test accuracy: 0.9789\n",
      "7 Train accuracy: 0.98 Test accuracy: 0.9729\n",
      "8 Train accuracy: 0.993333 Test accuracy: 0.9771\n",
      "9 Train accuracy: 0.96 Test accuracy: 0.9758\n",
      "10 Train accuracy: 0.993333 Test accuracy: 0.9778\n",
      "11 Train accuracy: 0.96 Test accuracy: 0.9824\n",
      "12 Train accuracy: 1.0 Test accuracy: 0.9782\n",
      "13 Train accuracy: 0.966667 Test accuracy: 0.9815\n",
      "14 Train accuracy: 0.98 Test accuracy: 0.9819\n",
      "15 Train accuracy: 1.0 Test accuracy: 0.984\n",
      "16 Train accuracy: 0.986667 Test accuracy: 0.9763\n",
      "17 Train accuracy: 0.993333 Test accuracy: 0.9841\n",
      "18 Train accuracy: 0.98 Test accuracy: 0.9807\n",
      "19 Train accuracy: 1.0 Test accuracy: 0.9837\n",
      "20 Train accuracy: 0.986667 Test accuracy: 0.9822\n",
      "21 Train accuracy: 0.98 Test accuracy: 0.9819\n",
      "22 Train accuracy: 0.98 Test accuracy: 0.9832\n",
      "23 Train accuracy: 0.98 Test accuracy: 0.9799\n",
      "24 Train accuracy: 1.0 Test accuracy: 0.983\n",
      "25 Train accuracy: 0.986667 Test accuracy: 0.9818\n",
      "26 Train accuracy: 0.986667 Test accuracy: 0.9831\n",
      "27 Train accuracy: 0.993333 Test accuracy: 0.9842\n",
      "28 Train accuracy: 1.0 Test accuracy: 0.9805\n",
      "29 Train accuracy: 1.0 Test accuracy: 0.9833\n",
      "30 Train accuracy: 0.993333 Test accuracy: 0.9846\n",
      "31 Train accuracy: 1.0 Test accuracy: 0.9841\n",
      "32 Train accuracy: 0.993333 Test accuracy: 0.9841\n",
      "33 Train accuracy: 1.0 Test accuracy: 0.9866\n",
      "34 Train accuracy: 0.986667 Test accuracy: 0.982\n",
      "35 Train accuracy: 1.0 Test accuracy: 0.9807\n",
      "36 Train accuracy: 0.993333 Test accuracy: 0.9843\n",
      "37 Train accuracy: 0.993333 Test accuracy: 0.9866\n",
      "38 Train accuracy: 0.993333 Test accuracy: 0.9828\n",
      "39 Train accuracy: 0.986667 Test accuracy: 0.9753\n",
      "40 Train accuracy: 0.973333 Test accuracy: 0.9845\n",
      "41 Train accuracy: 0.986667 Test accuracy: 0.9856\n",
      "42 Train accuracy: 1.0 Test accuracy: 0.9841\n",
      "43 Train accuracy: 0.98 Test accuracy: 0.9792\n",
      "44 Train accuracy: 0.986667 Test accuracy: 0.9875\n",
      "45 Train accuracy: 1.0 Test accuracy: 0.983\n",
      "46 Train accuracy: 0.973333 Test accuracy: 0.9842\n",
      "47 Train accuracy: 1.0 Test accuracy: 0.9853\n",
      "48 Train accuracy: 0.986667 Test accuracy: 0.9816\n",
      "49 Train accuracy: 0.993333 Test accuracy: 0.9826\n",
      "50 Train accuracy: 1.0 Test accuracy: 0.9869\n",
      "51 Train accuracy: 1.0 Test accuracy: 0.9853\n",
      "52 Train accuracy: 0.993333 Test accuracy: 0.9787\n",
      "53 Train accuracy: 1.0 Test accuracy: 0.9839\n",
      "54 Train accuracy: 1.0 Test accuracy: 0.9866\n",
      "55 Train accuracy: 0.993333 Test accuracy: 0.9856\n",
      "56 Train accuracy: 1.0 Test accuracy: 0.9848\n",
      "57 Train accuracy: 1.0 Test accuracy: 0.9854\n",
      "58 Train accuracy: 1.0 Test accuracy: 0.9859\n",
      "59 Train accuracy: 1.0 Test accuracy: 0.9873\n",
      "60 Train accuracy: 1.0 Test accuracy: 0.9868\n",
      "61 Train accuracy: 1.0 Test accuracy: 0.9862\n",
      "62 Train accuracy: 1.0 Test accuracy: 0.9842\n",
      "63 Train accuracy: 1.0 Test accuracy: 0.9854\n",
      "64 Train accuracy: 0.993333 Test accuracy: 0.9842\n",
      "65 Train accuracy: 0.986667 Test accuracy: 0.9863\n",
      "66 Train accuracy: 1.0 Test accuracy: 0.9867\n",
      "67 Train accuracy: 1.0 Test accuracy: 0.9835\n",
      "68 Train accuracy: 0.993333 Test accuracy: 0.9855\n",
      "69 Train accuracy: 1.0 Test accuracy: 0.9809\n",
      "70 Train accuracy: 1.0 Test accuracy: 0.9868\n",
      "71 Train accuracy: 0.993333 Test accuracy: 0.9853\n",
      "72 Train accuracy: 0.993333 Test accuracy: 0.983\n",
      "73 Train accuracy: 0.993333 Test accuracy: 0.9785\n",
      "74 Train accuracy: 1.0 Test accuracy: 0.9849\n",
      "75 Train accuracy: 1.0 Test accuracy: 0.9854\n",
      "76 Train accuracy: 1.0 Test accuracy: 0.9843\n",
      "77 Train accuracy: 0.986667 Test accuracy: 0.9837\n",
      "78 Train accuracy: 1.0 Test accuracy: 0.9846\n",
      "79 Train accuracy: 1.0 Test accuracy: 0.9881\n",
      "80 Train accuracy: 1.0 Test accuracy: 0.9839\n",
      "81 Train accuracy: 1.0 Test accuracy: 0.9864\n",
      "82 Train accuracy: 1.0 Test accuracy: 0.9831\n",
      "83 Train accuracy: 1.0 Test accuracy: 0.9851\n",
      "84 Train accuracy: 0.986667 Test accuracy: 0.9838\n",
      "85 Train accuracy: 1.0 Test accuracy: 0.9863\n",
      "86 Train accuracy: 1.0 Test accuracy: 0.9864\n",
      "87 Train accuracy: 1.0 Test accuracy: 0.9862\n",
      "88 Train accuracy: 1.0 Test accuracy: 0.984\n",
      "89 Train accuracy: 1.0 Test accuracy: 0.988\n",
      "90 Train accuracy: 1.0 Test accuracy: 0.9865\n",
      "91 Train accuracy: 0.993333 Test accuracy: 0.9853\n",
      "92 Train accuracy: 1.0 Test accuracy: 0.9855\n",
      "93 Train accuracy: 1.0 Test accuracy: 0.9869\n",
      "94 Train accuracy: 1.0 Test accuracy: 0.9823\n",
      "95 Train accuracy: 1.0 Test accuracy: 0.9853\n",
      "96 Train accuracy: 0.993333 Test accuracy: 0.9866\n",
      "97 Train accuracy: 1.0 Test accuracy: 0.9847\n",
      "98 Train accuracy: 1.0 Test accuracy: 0.9872\n",
      "99 Train accuracy: 1.0 Test accuracy: 0.9866\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons1 = 150\n",
    "n_neurons2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "hidden1 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons1, activation=tf.nn.relu)\n",
    "hidden2 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons2, activation=tf.nn.relu)\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell([hidden1, hidden2])\n",
    "outputs, states_tuple = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "states = tf.concat(axis=1, values=states_tuple)\n",
    "logits = fully_connected(states, n_outputs, activation_fn=None)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer RNN with Keras \n",
    "\n",
    "When stacking RNNs with Keras remember to set __return_sequences=True__ on hidden layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Evaluate IRNN...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.7986 - acc: 0.7263 - val_loss: 0.3711 - val_acc: 0.8834\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 46s - loss: 0.2963 - acc: 0.9078 - val_loss: 0.2352 - val_acc: 0.9262\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.1983 - acc: 0.9386 - val_loss: 0.2365 - val_acc: 0.9241\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 43s - loss: 0.1630 - acc: 0.9498 - val_loss: 0.1340 - val_acc: 0.9574\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 43s - loss: 0.1308 - acc: 0.9598 - val_loss: 0.1228 - val_acc: 0.9629\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 43s - loss: 0.1177 - acc: 0.9638 - val_loss: 0.0982 - val_acc: 0.9703\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.1038 - acc: 0.9684 - val_loss: 0.0941 - val_acc: 0.9697\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 43s - loss: 0.0924 - acc: 0.9710 - val_loss: 0.0973 - val_acc: 0.9699\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 46s - loss: 0.0815 - acc: 0.9748 - val_loss: 0.0835 - val_acc: 0.9726\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0755 - acc: 0.9764 - val_loss: 0.0702 - val_acc: 0.9764\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0693 - acc: 0.9785 - val_loss: 0.0776 - val_acc: 0.9764\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 43s - loss: 0.0634 - acc: 0.9802 - val_loss: 0.0731 - val_acc: 0.9779\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0584 - acc: 0.9814 - val_loss: 0.0804 - val_acc: 0.9743\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 47s - loss: 0.0517 - acc: 0.9839 - val_loss: 0.0683 - val_acc: 0.9787\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 46s - loss: 0.0510 - acc: 0.9840 - val_loss: 0.0594 - val_acc: 0.9827\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0478 - acc: 0.9848 - val_loss: 0.0596 - val_acc: 0.9814\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0451 - acc: 0.9858 - val_loss: 0.0576 - val_acc: 0.9828\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0419 - acc: 0.9865 - val_loss: 0.0597 - val_acc: 0.9824\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0393 - acc: 0.9877 - val_loss: 0.0673 - val_acc: 0.9806\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 45s - loss: 0.0364 - acc: 0.9887 - val_loss: 0.0619 - val_acc: 0.9818\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0359 - acc: 0.9887 - val_loss: 0.0689 - val_acc: 0.9786\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 45s - loss: 0.0331 - acc: 0.9891 - val_loss: 0.0646 - val_acc: 0.9820\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0315 - acc: 0.9899 - val_loss: 0.0656 - val_acc: 0.9803\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0287 - acc: 0.9908 - val_loss: 0.0535 - val_acc: 0.9835\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0265 - acc: 0.9915 - val_loss: 0.0647 - val_acc: 0.9826\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0274 - acc: 0.9911 - val_loss: 0.0612 - val_acc: 0.9817\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0249 - acc: 0.9921 - val_loss: 0.0576 - val_acc: 0.9823.\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 45s - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0551 - val_acc: 0.9835\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 48s - loss: 0.0227 - acc: 0.9927 - val_loss: 0.0559 - val_acc: 0.9840\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 45s - loss: 0.0216 - acc: 0.9932 - val_loss: 0.0537 - val_acc: 0.9838\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0181 - acc: 0.9940 - val_loss: 0.0582 - val_acc: 0.9852\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0193 - acc: 0.9935 - val_loss: 0.0504 - val_acc: 0.9863\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 45s - loss: 0.0165 - acc: 0.9946 - val_loss: 0.0528 - val_acc: 0.9851\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0173 - acc: 0.9940 - val_loss: 0.0631 - val_acc: 0.9839\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 46s - loss: 0.0171 - acc: 0.9943 - val_loss: 0.0646 - val_acc: 0.9816\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0152 - acc: 0.9948 - val_loss: 0.0580 - val_acc: 0.9870\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0163 - acc: 0.9950 - val_loss: 0.0652 - val_acc: 0.9837 ETA: 2s - loss: 0.0154 - acc: - ETA: 2s - loss: \n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0143 - acc: 0.9953 - val_loss: 0.0542 - val_acc: 0.9848\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0550 - val_acc: 0.9849\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0127 - acc: 0.9961 - val_loss: 0.0556 - val_acc: 0.9860\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 44s - loss: 0.0126 - acc: 0.9961 - val_loss: 0.0541 - val_acc: 0.9851\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 46s - loss: 0.0110 - acc: 0.9966 - val_loss: 0.0603 - val_acc: 0.9859\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 54s - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0664 - val_acc: 0.9847\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 47s - loss: 0.0107 - acc: 0.9962 - val_loss: 0.0614 - val_acc: 0.9860\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 48s - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0538 - val_acc: 0.9865\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 51s - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0677 - val_acc: 0.9843\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 64s - loss: 0.0102 - acc: 0.9967 - val_loss: 0.0621 - val_acc: 0.9844\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 51s - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0672 - val_acc: 0.9848\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 47s - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0709 - val_acc: 0.9845\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 45s - loss: 0.0081 - acc: 0.9975 - val_loss: 0.0604 - val_acc: 0.9853\n",
      "IRNN test score: 0.0603713211539\n",
      "IRNN test accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "batch_size = 150\n",
    "num_classes = 10\n",
    "epochs = 50 # instead of 100 (too much time)\n",
    "hidden_units_1 = 150\n",
    "hidden_units_2 = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('Evaluate IRNN...')\n",
    "a = Input(shape=x_train.shape[1:])\n",
    "b = SimpleRNN(hidden_units_1,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(),\n",
    "                    activation='relu' , return_sequences=True)(a)\n",
    "b = SimpleRNN(hidden_units_2,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(),\n",
    "                    activation='relu')(b)\n",
    "b = Dense(num_classes)(b)\n",
    "b = Activation('softmax')(b)\n",
    "optimizer = keras.optimizers.Adamax(lr=learning_rate)\n",
    "model = Model(inputs=[a], outputs=[b])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('IRNN test score:', scores[0])\n",
    "print('IRNN test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series\n",
    "\n",
    "Now let’s take a look at how to handle time series, such as stock prices, air temperature, brain wave patterns, and so on. In this section we will train an RNN to predict the next value in a generated time series. Each training instance is a randomly selected sequence of 20 consecutive values from the time series, and the target sequence is the same as the input sequence, except it is shifted by one time step into the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtesei\\AppData\\Local\\Continuum\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqkAAAEeCAYAAACpAYyXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ae9b69b7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_min, t_max = 0, 30\n",
    "\n",
    "n_steps = 20\n",
    "\n",
    "def time_series(t):\n",
    "    return t * np.sin(t) / 3 + 2 * np.sin(t*5)\n",
    "\n",
    "def next_batch(batch_size, n_steps,resolution = 0.1):\n",
    "    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)\n",
    "    Ts = t0 + np.arange(0., n_steps + 1) * resolution\n",
    "    ys = time_series(Ts)\n",
    "    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)\n",
    "\n",
    "t = np.linspace(t_min, t_max, (t_max - t_min) // resolution)\n",
    "t_instance = np.linspace(12.2, 12.2 + resolution * (n_steps + 1), n_steps + 1)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plt.title(\"A time series (generated)\", fontsize=14)\n",
    "plt.plot(t, time_series(t), label=r\"$t . \\sin(t) / 3 + 2 . \\sin(5t)$\")\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"b-\", linewidth=3, label=\"A training instance\")\n",
    "plt.legend(loc=\"lower left\", fontsize=14)\n",
    "plt.axis([0, 30, -17, 13])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"A training instance\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"w*\", markersize=10, label=\"target\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.0913747 , -1.81370459],\n",
       "       [-1.81370459, -0.2545896 ],\n",
       "       [-0.2545896 ,  1.35043982],\n",
       "       [ 1.35043982,  2.75633863],\n",
       "       [ 2.75633863,  3.76705941],\n",
       "       [ 3.76705941,  4.28205029],\n",
       "       [ 4.28205029,  4.31936936],\n",
       "       [ 4.31936936,  4.00977195],\n",
       "       [ 4.00977195,  3.56323364],\n",
       "       [ 3.56323364,  3.2161207 ],\n",
       "       [ 3.2161207 ,  3.17195748],\n",
       "       [ 3.17195748,  3.55030744],\n",
       "       [ 3.55030744,  4.35629752],\n",
       "       [ 4.35629752,  5.47826041],\n",
       "       [ 5.47826041,  6.71408441],\n",
       "       [ 6.71408441,  7.81983055],\n",
       "       [ 7.81983055,  8.56872404],\n",
       "       [ 8.56872404,  8.80608565],\n",
       "       [ 8.80608565,  8.48676122],\n",
       "       [ 8.48676122,  7.68589142]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next_batch(1, n_steps)\n",
    "np.c_[X_batch[0], y_batch[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an OuputProjectionWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tMSE: 20171.0\n",
      "100 \tMSE: 858.855\n",
      "200 \tMSE: 422.972\n",
      "300 \tMSE: 199.725\n",
      "400 \tMSE: 90.7315\n",
      "500 \tMSE: 56.7009\n",
      "600 \tMSE: 53.6289\n",
      "700 \tMSE: 62.3246\n",
      "800 \tMSE: 50.8053\n",
      "900 \tMSE: 53.6313\n",
      "[[[-3.49738097]\n",
      "  [-2.51607442]\n",
      "  [-1.16632104]\n",
      "  [ 0.60030717]\n",
      "  [ 2.15503573]\n",
      "  [ 3.01053429]\n",
      "  [ 3.41778255]\n",
      "  [ 3.32022381]\n",
      "  [ 2.88544369]\n",
      "  [ 2.23823047]\n",
      "  [ 1.69417715]\n",
      "  [ 1.57740271]\n",
      "  [ 1.955423  ]\n",
      "  [ 2.77756476]\n",
      "  [ 3.91065073]\n",
      "  [ 5.11356974]\n",
      "  [ 6.10596704]\n",
      "  [ 6.64041662]\n",
      "  [ 6.67867994]\n",
      "  [ 6.1244235 ]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),\n",
    "    output_size=n_outputs)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "n_outputs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(outputs - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_iterations = 1000\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "    \n",
    "    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_new})\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEeCAYAAABv8mXfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ae9d014208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Testing the model\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"w*\", markersize=10, label=\"target\")\n",
    "plt.plot(t_instance[1:], y_pred[0,:,0], \"r.\", markersize=10, label=\"prediction\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without using an OutputProjectionWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tMSE: 16.1839\n",
      "100 \tMSE: 0.396811\n",
      "200 \tMSE: 0.124991\n",
      "300 \tMSE: 0.0574633\n",
      "400 \tMSE: 0.0538617\n",
      "500 \tMSE: 0.059357\n",
      "600 \tMSE: 0.047703\n",
      "700 \tMSE: 0.0494709\n",
      "800 \tMSE: 0.0483082\n",
      "900 \tMSE: 0.0500153\n",
      "[[[-3.46322608]\n",
      "  [-2.48452091]\n",
      "  [-1.10020792]\n",
      "  [ 0.58717555]\n",
      "  [ 2.00378942]\n",
      "  [ 3.0813272 ]\n",
      "  [ 3.52336788]\n",
      "  [ 3.42467713]\n",
      "  [ 2.84561419]\n",
      "  [ 2.11835122]\n",
      "  [ 1.66789067]\n",
      "  [ 1.52136159]\n",
      "  [ 1.86960983]\n",
      "  [ 2.6718235 ]\n",
      "  [ 3.84957361]\n",
      "  [ 5.06223202]\n",
      "  [ 6.07499313]\n",
      "  [ 6.60482883]\n",
      "  [ 6.601717  ]\n",
      "  [ 5.99486113]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "n_outputs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "stacked_outputs = fully_connected(stacked_rnn_outputs, n_outputs, activation_fn=None)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_iterations = 1000\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "    \n",
    "    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_new})\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEeCAYAAABv8mXfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEspiAvIpLFFORFRLKYgryISBb7/8y66/r7CMZ4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ae9d084a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Testing the model\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"w*\", markersize=10, label=\"target\")\n",
    "plt.plot(t_instance[1:], y_pred[0,:,0], \"r.\", markersize=10, label=\"prediction\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate IRNN...\n",
      "Train on 50000 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 13s - loss: 1.8889 - mean_squared_error: 1.8889 - val_loss: 0.4352 - val_mean_squared_error: 0.4352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aea8dd7b38>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "def ts_next_batch(batch_size, n_steps,resolution = 0.1):\n",
    "    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)\n",
    "    Ts = t0 + np.arange(0., n_steps + 1) * resolution\n",
    "    ys = time_series(Ts)\n",
    "    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "batch_size = 50\n",
    "hidden_units = 100\n",
    "learning_rate = 0.001\n",
    "n_inputs = 1\n",
    "n_outputs = 1\n",
    "n_steps = 20\n",
    "\n",
    "print('Evaluate IRNN...')\n",
    "a = Input(shape=(n_steps,n_inputs))\n",
    "b = SimpleRNN(hidden_units,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(),\n",
    "                    activation='relu' ,  return_sequences=True)(a)\n",
    "b = keras.layers.core.Reshape((-1,n_neurons))(b)\n",
    "b = Dense(1,activation=None)(b)\n",
    "b = keras.layers.core.Reshape((n_steps, n_outputs))(b)\n",
    "optimizer = keras.optimizers.Adamax(lr=learning_rate)\n",
    "model = Model(inputs=[a], outputs=[b])\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mean_squared_error'])\n",
    "\n",
    "X_batch, y_batch = ts_next_batch(batch_size*1000, n_steps)\n",
    "x_test, y_test = ts_next_batch(batch_size, n_steps)\n",
    "model.fit(X_batch, y_batch,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.70384717]\n",
      "  [-3.05007172]\n",
      "  [-2.39496326]\n",
      "  [-0.14787099]\n",
      "  [ 1.67567396]\n",
      "  [ 2.97915959]\n",
      "  [ 3.60192847]\n",
      "  [ 3.50552845]\n",
      "  [ 2.90764904]\n",
      "  [ 2.10811305]\n",
      "  [ 1.44303656]\n",
      "  [ 1.22440231]\n",
      "  [ 1.58337164]\n",
      "  [ 2.46381497]\n",
      "  [ 3.68792629]\n",
      "  [ 4.98927641]\n",
      "  [ 6.08448458]\n",
      "  [ 6.71567392]\n",
      "  [ 6.7268734 ]\n",
      "  [ 6.1052804 ]]]\n"
     ]
    }
   ],
   "source": [
    "X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "y_pred = model.predict(X_new,verbose=0)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEeCAYAAABv8mXfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aea779a588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Testing the model\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"w*\", markersize=10, label=\"target\")\n",
    "plt.plot(t_instance[1:], y_pred[0,:,0], \"r.\", markersize=10, label=\"prediction\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "If you build a very deep RNN, it may end up overfitting the training set. To prevent that, a common technique is to apply dropout. You can simply add a dropout layer before or after the RNN as usual, but if you also want to apply dropout between the RNN layers, you need to use a DropoutWrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tMSE: 13.1851\n",
      "100 \tMSE: 5.1124\n",
      "200 \tMSE: 3.35787\n",
      "300 \tMSE: 3.95527\n",
      "400 \tMSE: 3.24614\n",
      "500 \tMSE: 4.12\n",
      "600 \tMSE: 3.30315\n",
      "700 \tMSE: 2.72216\n",
      "800 \tMSE: 2.72711\n",
      "900 \tMSE: 3.54013\n",
      "1000 \tMSE: 2.55609\n",
      "1100 \tMSE: 2.66153\n",
      "1200 \tMSE: 2.37055\n",
      "1300 \tMSE: 2.55441\n",
      "1400 \tMSE: 2.08881\n",
      "1500 \tMSE: 1.91476\n",
      "1600 \tMSE: 3.05383\n",
      "1700 \tMSE: 2.6395\n",
      "1800 \tMSE: 2.75546\n",
      "1900 \tMSE: 2.03541\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_layers = 3\n",
    "n_steps = 20\n",
    "n_outputs = 1\n",
    "\n",
    "keep_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "\n",
    "is_training = True\n",
    "\n",
    "def deep_rnn_with_dropout(X, y, is_training):\n",
    "    if is_training:\n",
    "        multi_layer_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicRNNCell(num_units=n_neurons), input_keep_prob=keep_prob) for _ in range(n_layers)],)\n",
    "    else: \n",
    "        multi_layer_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) for _ in range(n_layers)],)\n",
    "        \n",
    "    rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "\n",
    "    stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "    stacked_outputs = fully_connected(stacked_rnn_outputs, n_outputs, activation_fn=None)\n",
    "    outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "    return outputs, loss, training_op\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "outputs, loss, training_op = deep_rnn_with_dropout(X, y, is_training)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_iterations = 2000\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if is_training:\n",
    "        init.run()\n",
    "        for iteration in range(n_iterations):\n",
    "            X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            if iteration % 100 == 0:\n",
    "                mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                print(iteration, \"\\tMSE:\", mse)\n",
    "        save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "    else:\n",
    "        saver.restore(sess, \"/tmp/my_model.ckpt\")\n",
    "        X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "        y_pred = sess.run(outputs, feed_dict={X: X_new})\n",
    "        \n",
    "        plt.title(\"Testing the model\", fontsize=14)\n",
    "        plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "        plt.plot(t_instance[1:], time_series(t_instance[1:]), \"w*\", markersize=10, label=\"target\")\n",
    "        plt.plot(t_instance[1:], y_pred[0,:,0], \"r.\", markersize=10, label=\"prediction\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEeCAYAAABv8mXfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8VOW99/3PL0O2QAa0onV7awXKLhsQQxAwVlRAVIyi\nVoWtVn1pW2tb7GFb6SNPS6uF3efZrdsztty2ttvbUtxCq1Vq5NkesFoUxYL1AOU2Bc96RxRKEg5J\n+D1/XDMwCclkyKzJHPJ9v17rtTJr1uGaRfjNleu61u8yd0dEREpTWb4LICIiuaMgLyJSwhTkRURK\nmIK8iEgJU5AXESlhCvIiIiVMQV4Kkpn9u5mtzuP13zezr+fr+lEws8fMbOF+7D/CzNzMRueyXNKz\nFOSlU4n/8OmW/4zgGp0Fln8DpmV7/gyu/1Uz+zDX1xHJlz75LoAUtMNTfp4O/Lzdtu25urC7NwAN\nuTq/SG+hmrx0yt3fTy7Alvbb3H0rgJkNNrMlZrbFzDab2UNmNjR5HjMbambLzOxjM2s0s9fM7Hwz\n6wusS+z2cqJG/2jimDbNNWZ2n5ktNbPvmNl7iev83MwOSNlnoJn9JnGN98zs2nRNFmZ2BvAzYFDK\nXydzUnapMLNfmtk2M3vLzL7Z7viDzexuM6s3s7+b2RNmVpXuniaagf5vM/u1mTWY2RuJe3Fw4vM1\nmNl6M5vc7ripZvaCme1MfLafmFl5yvsDEudMfvbZHVy7r5ndZGbvJPZbZWanpCuvFD8FecmKmQ0A\nVgAfAycBJxK+EP47JQDfBRhwMnAMMBv4u7vvSBwDMJnwV8LFaS53GjAEmAJcBlwEzEp5/3bgeODs\nxL4TgQlpzvcEcB3wUeLahwN3pLw/G3geGAvcBtxmZscmPncMeBQYBNQA44DVwBNmdmiaawJcCzwF\nVAEPAfcCvwZ+l7jWC8AiM/uHxLWGAH8AngPGAF8DvgDckHLO2wj38hzg9MTPx7W77qLEtguBSuC/\ngFozG9lFeaWYubsWLV0uwIzw67LP9lnAK+22lQPbgHMSrzcA13Vy3hGAA6Pbbf93YHXK6/uAOqAs\nZdu9wLLEzwcDLcDnUt4/MFGOhWk+11eBDzvY/j7wq3bb3gJmJ34+k/Dl8A/t9lkPfDPN9dqcFzgk\n8fl/0tk9AW4CXgWsXbmbEvc6+dkvSHn/IEJz18LE61FAK3BYu/I8Ctyc7t9CS3EvapOXbI0DRphZ\n+/bz/sCwxM+3EmrB5wCPA79z97XduNYr7r475fW7wD8nfv4MECPUvAFw961mtr4b10n6S7vX7wKf\nTPw8jvAl8pGZpe7Tl72fu8vzuvuHZtYKvJzy/geJdfJaI4GVnojECc8A/YChwCcIn/3ZlPNuMbN1\nKfuPI/zlXteuvAcAO7sorxQxBXnJVhmwCri8g/c+BHD3n5rZMkLt91Rgjpn9wN3/fT+v1dzutbO3\nydFStkUl3fXKgLeBqR0ct3U/z9t+W/IzpH62zj6Xs/ezp1OWuMbYDs7VmMHxUqTUJi/Z+jMwHPjA\n3V9vt2xJ7uTub7r7QnefAfwIuCrx1q7EOpZlOTYQmiP2tEOb2UBCE0Q6u7p57T8D/wPY2cHnru/G\n+dJ5DZhobavgJxJGN21i72c/PvmmmR1I+AsgtbzlwCEdlPe9iMsrBURBXrJ1D6Hd+0EzOykxkmaS\nmd1mZoMBzGyBmZ2eeO9YQqfoa4nj3yME2jPM7JOJwLzf3P0jQuflTWY22cyOBn4J7CZ97X4TcGCi\nzIeYWb8ML/kIIXA+lPhsQ8zsBDP7NzOr7s5nSOMOQhPQbYnnCs4F5gO3uHtz4rPfS/jspySeOfhP\nwmcHwN1fBn5L6NA9L/FvMcHMrjOzsyMurxQQBXnJirv/nVCrfJcwOmQd8CtCm3yy2aKcMFRxHaGj\n7w3gS4njtwPXAF8nBPz7syjONwkjUx4BHgNWAq8AO9Ic82SivL8D6oFvZXIhd28ljGJZSQioGwid\nw0MJnauRcfdNwFnACcBLwP9MlPmGlN2+RWiTf5jw2Z8jpX8i4RLgN8DNwF8JI3uOB96MsrxSWKxt\nX45I6UjUyt8GfuDud+a7PCL5oI5XKRlmdhyhJr2aMPLle4S/Ipbms1wi+aQgL6XECA83DSe0868B\nTnL3D9IeJVLC1FwjIlLC1PEqIlLC8t5cc8ghh/iQIUPyXQwRkaLy4osvfujuXeVJyn+QHzJkCKtX\n521uCBGRomRmb2Syn5prRERKmIK8iEgJU5AXESlheW+T70hzczNvv/02O3akexpd9kffvn058sgj\nKS8v73pnkRJXVwc33QQPPQR33AHf+Aaccw5cey0M6yJRdDbH5kW+E9qPGzfO2/vb3/7m9fX1vnv3\n7n3ek/23e/dur6+v97/97W/5LopI3j3yiHv//u7l5e6XXhq2XXJJeN2/f3g/F8dGjZRJddItBdlc\ns2PHDgYNGkS7yQ2km8yMQYMG6S8j6fXq6mDGDGhqguZm+OIXw/YvfjG8bmoK79fVRXtsPhVkcw2g\nAB8x3U8R2LULGlOmSNmZmBNr4kRIffh/fQfziWVzbD4VZE1+f9TVwaxZMHAglJWF9axZhfdtKiL5\nN3t220B9wAFt1xDev/baaI/Np6IO8rW1UFkJv/gFbNsWvk23bQuvKyvD+911wgkndOu4Bx98kNde\ne63rHUUke62tsGwZzJ8f1q2taXevrYXp09sG61SNjXDWWfDoo9Eem02Zs5ZJw30ul446Xl977bUu\nOx1efz10dITQ3vHSv3/YryddfvnlvmTJkp69aIYyua8iRaOlxX3qVPd43N0srKdODds7MWBAiA1n\nneXe1NT2vaamsB3cBw6M9thsytwZirnjNRM33RQ6O9JpboZbbune+ePxOAArVqxg8uTJzJgxgxEj\nRnDJJZfgiQa4OXPmMGrUKCorK5k9ezYrV67koYce4jvf+Q5VVVXU1dXx85//nAkTJjBmzBguuOAC\nmpqaALjiiiv45je/yQknnMCnP/1pli7dm/L8Jz/5Cccccwxjxoxhzpw5ANTV1XHGGWcwbtw4Tjrp\nJNYXWsOfSE+rrYVVq6ChIdTrGhrC6zR/wl96KZSXw0EHQcvOVlp/v4xdP5hP6++X0bKzlYMOCu9f\ndlkXx7aEpalp78/pjs2mzFnL5Jsgl0t3a/LJb9WulrTfqmlUVFS4u/uTTz7pAwcO9LfeestbW1v9\n+OOP96effto3b97sw4cP3zPM8+OPP3b3fWvyH3744Z6fv/e97/ntt9++Z78ZM2Z4a2urv/rqqz5s\n2DB3d3/kkUf8s5/9rDc2Nrq7++bNm93d/ZRTTvENGza4u/tzzz3nU6ZM2e/PpJq8lJR580JtOPU/\nvJn7/PmdHpJsAXjiv1t89ylTvaVf3Fsxb+kX992nTPXH/7+WTlsA9hz7hHvLzhb/37c87L/69Dz/\n37c87C07W/zxxzNoPehGmTtDhjX5gh1d05WGhmj3S+e4447jyCOPBKCqqopNmzZx/PHH07dvX668\n8krOOusspk+f3uGxr7zyCnPnzmXLli00NDQwbdq0Pe997nOfo6ysjFGjRvHBB2Fei8cee4wvfOEL\n9O/fH4CDDz6YhoYGVq5cycyZM/ccuzPZtS/SS733j2P5RKyCvi17/5PviFXw8WFVHN7JMcOGwdKl\n0P+Ptex6ehUHNCeO3d7AzqdXUTGxlqVLp3f4UFPy2IatrdR9ZhqHv7mKy2ik6ZoK/nZLNY23LWfp\n0lj6B6LGjoWKiraBqaICqqr2+/NnqmibaxKtKZHtl84BKd3nsViMlpYW+vTpw/PPP88FF1zAgw8+\nyBlnnNHhsVdccQULFizg5Zdf5vrrr28zVj31vJ5oAnL3fYY77t69m4MOOoi1a9fuWdatW5f9BxMp\nUrW1MPxbNaxsrWZHnziYsaNPnJWt1Qz/Vk3a1o+aGvjMtjWUN7ftQS1vbuSfGtZSU5P+2PH1tRz1\n3ioG0EAMZwANfOq9VYyvr0177J4TVFeHwGQW1tXVdH1g9xVtkE+2j6XTZftYFhoaGti6dStnnnkm\nt956K2vXrgVgwIABbNu2bc9+27Zt4/DDD6e5uZlFixZ1ed7TTz+dX/7yl3va7j/66CMGDhzI0KFD\nWbJkCRC+CF566aUcfCqRwpd8KKlhe4zTfDkbfrgY5s3jrzcs5jRfTsP2WJcPJR08dSxl8Yo228ri\nFQw6pesa9eHvr6FvS9sviL4tjRz+wdquCx+LwfLlsDiUmcWLw+tYrOtjuynyIG9mF5nZOjNrNLM6\nMzsp6mtAGIuaSZC/5ppcXD0E7+nTp1NZWcmkSZO4JdHDe9FFF3HjjTcyduxY6urqmD9/PtXV1Zx2\n2mmMGDGiy/OeccYZnHPOOYwfP56qqir+4z/+A4BFixZx9913M2bMGI4++mh+//vf5+aDiRS45ENJ\n7tDqMf752ukwdy4jZk+n1WO4h/fTDszIpkadbHJJtT9NLrFYGIs5d25Y5zDAA9F2vAKnAW8AxxO+\nQI4Ajkh3THc7Xt3b5pFI7cfIRx6JYqCOVykFZ57p3tCQfp+GhrBfWi0t7g8/HDo9H34482GMEQ6D\nzAZ56nj9ITDP3Z9LvH4n4vO3UVMDf/lLGCZ5772hLyMeD00011xToBnhRCQryYeSli3bt0INex9K\nevrpLk6UrFF3Mmgi7XHLl4eCrF0bavA1NbmvkXeTuXvXe2VyIrMYsB34AXAl0Bd4EPiOu29vt+9V\nwFUARx111Lg33mg7i9W6desYOXJkJOWSvXRfpRQMHBiebD/rLFiyBPr12/ve9u0wcyb84Q9hv61b\n81fOXDOzF919fFf7RdkmfxhQDswATgKqgLHA3PY7uvtd7j7e3ccfemiX89CKiOwRyUNJvUiUQT5Z\nW7/D3d9z9w+Bm4EzI7yGiPRyyUEXX/oS9O8fmmzPPTes+/cPqX9zOeii2EQW5N39Y+BtIJr2HxGR\nDux5KKkBrrsOxo+Hxx6DCRNgzpzQJr90qfrkkqIeQvkr4Btm9kkz+wTwr8CyiK8hIr1cTQ2MGgU7\ndsCAASHNeDwe2uRHjcrps0VFJ+ogPx94AdgArAPWAD+K+Bo5t2XLFn7605/m/DorVqxg5cqVOb+O\nSEHrZurdYcNgwYLQudraGtYLFqgG316kQyjdvRmYlViKVjLIz5qV2cdIjkctK9u/78wVK1YQj8e7\nnbtepOi1tsK0aSETY2NjGBNZXZ3zp0B7k6JNa5BLc+bMoa6ujqqqKq655hqmTp3KscceyzHHHLPn\nSdNNmzYxcuRIZs2axbHHHstbb73F3XffzfDhw5k8eTJf/vKX+frXvw5AfX09F1xwARMmTGDChAn8\n6U9/YtOmTSxcuJBbbrmFqqoqnu5yUK9ICcpH6t3eJpMnpnK5ZPPEa65s3LjRjz76aHd3b25u9q1b\nt7q7e319vQ8bNsx3797tGzdudDPzZ5991t3d33nnHR88eLBv3rzZd+3a5SeeeKJfffXV7u5+8cUX\n+9NPP+3u7m+88YaPGDHC3d2vv/56v/HGG3vsc+X7vorsI8LUu70NpZ5quKe4O9/97nf54x//SFlZ\nGe+8886etMCDBw/m+OOPB+D5559n0qRJHHzwwQDMnDmTDRs2ACF9cOqUgH//+9/bJDET6bXykHq3\nt1GQ78KiRYuor6/nxRdfpLy8nCFDhuxJF1yR8ky1p3lyePfu3Tz77LP0S300T0SoG15D84HVfKpx\nFf1ppIkK3jqwmvLhNaj/NBpqk+9AarrgrVu38slPfpLy8nKefPJJ2qdgSDruuON46qmn+Pjjj2lp\naeG3v/3tnvdOP/10FixYsOd1Z2mJRXqT2lqoHBuj6oPlLJy0GJs3j5+dvJiqD5ZTOTamZvmIKMh3\nYNCgQUycOJHRo0ezdu1aVq9ezfjx41m0aFGn6YKPOOIIvvvd71JdXc2pp57KqFGjOPDAAwG4/fbb\nWb16NZWVlYwaNYqFCxcCcPbZZ/PAAw+o41V6nWRO+KYm2NkS49gfhNS7x/5gOjtbYjQ10WVOeMlM\nZAnKumv8+PG+evXqNtuKNZFWQ0MD8XiclpYWzjvvPL74xS9y3nnn5btYexTrfZXSs24dpP4q7twJ\nBxywd520fj1kMA1Dr5SPBGW93g033EBVVRWjR49m6NChfO5zn8t3kUQK0uzZYVh8UjKwpwb4xsaQ\np0ayo47XCCVncRKR9CLLCS9dUk1eRHpcPA4rVsCFF4Z8M6m2bw/bn3oq7CfZUZAXkR6nnPA9R0Fe\nRHqccsL3HAV5EclON7JIKid8z1GQ7yHxROPiu+++y4wZM9Lue+utt9LU1LTn9ZlnnsmWLVtyWj6R\nbklmkbz4Yrj++rCeNi2jQK+c8D2jNMbJt7aG7vo1a0IujB6aOb21tZVYhteJx+M0pObnSGPIkCGs\nXr2aQw45JJvi7UPj5CVyy5aFwJ76ux2Pw+LFYfiM5EzvGSefRU0inU2bNjFixAguv/xyKisrmTFj\nBk1NTQwZMoR58+Zx4oknsmTJEurq6jjjjDMYN24cJ510EuvXrwdg48aNfPazn2XChAl8//vfb3Pe\n0aNHJ4reyuzZsznmmGOorKzkjjvu4Pbbb+fdd99lypQpTJkyBQhB/8MPPwTg5ptvZvTo0YwePZpb\nb711zzlHjhzJl7/8ZY4++mhOP/10trcfsiCSC2vWtB3wDuF1InWHFIBMUlXmcsk61fDDD7vH421T\nlcbjYXsWNm7c6IA/88wz7u7+hS98wW+88UYfPHiw//jHP96z3ymnnOIbNmxwd/fnnnvOp0yZ4u7u\nZ599tt9zzz3u7r5gwQKvqKjYc95kGuOf/vSnfv7553tzc7O7u2/evNnd3QcPHuz19fV7rpF8vXr1\nah89erQ3NDT4tm3bfNSoUf7nP//ZN27c6LFYzNesWePu7jNnzvR77713n8+kVMMSuRz9/5OukWGq\n4eKvyeewJvGpT32KiRMnAnDppZfyzDPPAHDhhRcCIY3BypUrmTlzJlVVVXzlK1/hvffeA+BPf/oT\nF198MQCXdTIO7LHHHuOrX/0qffqEZ9KSaYo788wzz3DeeedRUVFBPB7n/PPP35PzZujQoVQl0rOO\nGzeOTZs2ZfHJRTJUUxNmcorHwSysq6vVoF5Aiv+J1xzmozazDl8nUwzv3r2bgw46aE9Wya6Ob8/d\nu9yn/f6dOSDlefBYLKbmGukZsViYqq+2NlSsqqp6rE9MMlP8Nfkc1iTefPNNnn32WQAWL17MiSee\n2Ob9gQMHMnToUJYsWQKEIPzSSy8BMHHiRO677z4g5KTvyOmnn87ChQtpaWkB4KOPPgI6T0F88skn\n8+CDD9LU1ERjYyMPPPAAJ510UtafUyQrsVjoZJ07N6wV4AtK8Qf5ZE1i8WKYNy+sI5oEeOTIkdxz\nzz1UVlby0Ucf8bWvfW2ffRYtWsTdd9/NmDFjOProo/fMAXvbbbdx5513MmHCBLZu3drh+a+88kqO\nOuooKisrGTNmDL/5zW8AuOqqq6ipqdnT8Zp07LHHcsUVV3DcccdRXV3NlVdeydixY7P+nCJSunIy\nhNLMPgO8DCx190vT7VuoqYY3bdrE9OnTeeWVV/JajigVwn3dL3kaGitSDPI9hPJO4IUcnVt6gxwN\njZVo1dXBrFlw5JHwwANhPWuWJvsoJJEHeTO7CNgCPB71uXvSkCFDSqoWX3Rqa2HVqtCh7h7Wq1ah\nOeEKR20tVFbCL34BU6bAeefB5MnhdWWl/qkKRaRB3swGAvOArFP956IZqTfL2/3sRl4TQA/ZFLjU\n6fuam0NCMQjr5mY0fV8BiXoI5Xzgbnd/K93QQDO7CrgK4Kijjtrn/b59+7J582YGDRq0X0MMpWPu\nzubNm+nbt2/PXjjZ5LJqVQjQFRVh5FMmHeM5HBor2du1q+138M6dYT1xYvjDKynxALjkUWQdr2ZW\nBSwCxrr7LjO7Afin7nS8Njc38/bbb7Njx45Iyibhi/PII4+kvLy85y6aTV6TbL4gJOfOOgvuv7/j\nWZ2SGhvhX/4F/vCHnitXb5Jpx2uUNfnJwBDgzUTtOw7EzGyUux+7PycqLy9n6NChERZN8iJdk0tX\nQV4P2RQ0Td9XPKIM8ncB96W8nk0I+vsOLpde4b1/HMsnYhX0bdlbk98Rq+Djw6o4PJMTJB+yUTbD\ngpM6fd+SJdCv3973UqfvGzgwb0WUhMg6Xt29yd3fTy5AA7DD3eujuoYUj9paGP6tGla2VrOjT3ga\neUefOCtbqxn+rZouR15oaF5h0/R9RSSTLGa5XDrKQinF7fXX3fv3DwkJy2jxl370sPv8+b723x72\nMlocwvuvv97x8Y88Et4vL3e/9NKw7ZJLwuv+/cP7kl/Jf+MnnnBvaXF/8UX3U08N65YW98cfT/9v\nLNkjwyyUBTlpiBS3desg9cHanTvhgAP2rpPWr4cRI9oeW1cXxlgnJ8Z64okwBvuJJ2Dq1LAtOSeo\npobLr9raUGt/6im4+eYwqqasDL79bTj5ZOjTR8kocynfT7xKLzZ7dtv+1mRgTw3wjY1hMuf2kkPz\nksnJTzghbE8OzXMP7zc35678khlN31ccVJOXyJWVwaRJmY28aP9slIbmiWRGNXnJm9SRF+3T2qeO\nvEjMbd5Gcmhe+5GXSckviEcfjbzYIiVJQV461d0RLtmMvMjmC0JE9qUgLx3KJvnUtdeGIP6lL+3t\nJD333LDu3z/kNykvh2uu2fdYDc0TiZaCvOwj2+RTw4bB0qUhm8F118H48fDYYzBhAsyZE5pcli7t\neHRMNl8QqeW/+qutXD5oGa9dPJ/LBy3j6q+2aoy99ErFP8erRC6K5FM1NSHYLl8eRl40NLQdedHZ\n8Mf2XxDJoXkTJuwdmtfZFwSEvzD+5YJWfr9jGifEVtH3vxpZGKvg2buqqfpfy7n/tzGN+pBeRTV5\n2Uc2QyBTDRsGCxbA1q1hFM3WreF1V+Pbuzs0L/kXyKTttUzwVSGdgjv9WhqY4KuYtL02s/S33U2P\nLFKAFORlH21GuHQQ8HpihEt3viCSf4Esm7eGAdZ2eM4Aa2TZ/LVdj7HPdkaqIvyCUPNWicvksdhc\nLkprUHgGDAiPHZ19Zou3TJ7qHo+7m7nH494yeaqffWZITTBwYL5L2taZZ7o3NLj7ww+HMu99fiq8\nfvhhb2gI+3UqzbFdamlxn9r2fvnUqWF7gXrkEfd4vxZ/3Kb69j6h3E194v64TfV4vxalkChgZJjW\nQDV52UdyhMvJjbWUvdB2Cr6yF1ZxcmNtQY5w2fMXyMk1Ifd8PCRGIx6H6moaT67p+i+QbGakKrIp\nCyNr3pKCpiAv+0iOcDn/02ugqV3Aa2rk/E+v7XKESz7sGWP/+RjbH1weJieZNw8WL2b7g8u58POx\nrsfYJ2ekSpXpjFRFNmVhJM1bUvAU5GUfyREu9UeOZVeftgFvV58K6o+sSjvCJV/ajLH3GC1nTKfp\n23NpOWM6LR7LbIx9Tcd/BWQ0JCebL4g82NPBnqbcmXSwS4HLpE0nl4va5AvX639t8XVHTPVtxL0F\n823Efd0RU/31vxZmG3Nk6W9bWkIb/Pz5YZ1pm3qRtcmbuU+e7N6wteNyN2xt8UmT3MvK8l1S6QhK\nNSyRaG0tqin48p7+toju18CBsG1bGCm15L5W+q3YW+7tk2uYeVGMP/wh7Ld1a75LK+0pQZlEIzkF\n39y5YV2gASsp2/S3Wc9IVUT3K5LmLSl8mVT3c7mouUYKRW+bkUqzOxU3NIRSQHOlZirbfD3FKJsc\nQ1I8FORLWDaZJHub3jojlWZ3Kn3qeC1Rmit1/5TEjFTJTt81a8KwyALu9JXsZdrxqiyUJSqKTJK9\nSfJp2UymLCxIyZw7q1aFwlZUhPH9y5cr0PdykTXXmNkBZna3mb1hZtvMbI2Z6Y+9PIkqk2RvUewz\nUr33y1p2PNU2pcKOp1bx3i/VJtfbRdkm3wd4C5gEHAh8H7jfzIZEeA3JkOZK3T/FPCNVbS38/Oo1\nlLe0/ccub2nkrqvXqu+ll4ssyLt7o7vf4O6b3H23uy8DNgLjorqGZK7Ya6Y9LaoZqXp6JFNyVNDz\nzWNpom07UxMVvNBcVXKjgmT/5Gx0jZkdBgwHXu3gvavMbLWZra6vr89VEXq1Yq6Z5kO2wwnzNZJp\nT5KxlhoGTK3GK0LOHa+IM2BqNctaakpyVJDsh0wG0+/vApQDjwH/s6t99TBUbuhBl+55/XX3q68O\nufLLysL66qvT36fkvU4OtnziibD98cf3bsvVvd6TQ9+905w7XebQl6JEvnLXmFkZ8BtgIHCuu6et\nQ2gIZe7kPY9LL7FuHYwcuff1zp2hgzu5Tlq/HkaM6Pw8dXVw003w0ENwxx3wjW/AOeeEpqTO/oIo\nK4NJkzIbFVQEk1TJfshL7hozM+Bu4DDggq4CvOSWHnTpGVGMZOpuc4/6XqQrUbfJ/wwYCZzt7tu7\n2llyr7uTaUvmsh3JlE1KBfW9SFeiHCc/GPgKUAW8b2YNieWSqK4hUoiyrU1nk1IhilFBUtqiHEL5\nhrubu/d193jKsiiqa4gUomxr09k09yjJmHRFCcpEspRtbTrb5h71vUg6SlAmEoFsRjK1maFpCfTr\nt/e97dth5kw0Q5PsQzNDifSgbGrT6jyVXFKQF4lId0cyqfNUcknNNSIFINnc88cnW1l/Sy1VrOEl\nG8uIa2o4aXJMD67JPpRPXqSI1NRA3YZWPvO1aRzJKvrRyHav4O3/qqb8K8sZNlw54aV71FwjUiCG\nbahlxNZVxGkghhOngRFbVzFsg3IFS/cpyBcBTcbdS6xZs+84ysZGWLs2P+WRkqAgX+A0GXcvMnbs\nvlnGKiqgqio/5ZGSoCBfwLLJaSJFqKYmzMsaDznhicfDa/W4ShbU8VrANBl3LxOLhYm3a2tDE01V\nVQjwmohbsqCafAHTZNy9UCwWchzMnRvWCvCSJQX5AqbJuEUkWwryBUwTQohIthTkC5hymohIthTk\nC5hymohIthTkC5gmhBCRbCnIFzhNCCEi2VAWShGRIqRJQ0REREFeRKSURRrkzexgM3vAzBrN7A0z\n+3yU5xc5na4WAAAOcUlEQVQRkf0Tde6aO4FdwGFAFfAHM3vJ3V+N+DoiIpKByGryZlYBXAB8390b\n3P0Z4CFAj+qIiORJlM01w4FWd9+Qsu0l4Oj2O5rZVWa22sxW19fXR1gEERFJFWWQjwNb223bCgxo\nv6O73+Xu4919/KGHHhphEUREJFWUQb4BGNhu20BgW4TXEBGR/RBlkN8A9DGzz6RsGwOo01VEJE8i\nC/Lu3gj8DphnZhVmNhE4F7g3qmuIiMj+ifphqFlAP+D/AIuBr2n4pIhI/kQ6Tt7dPwI+F+U5RUSk\n+5TWoIfU1cGsWXDkkfDAA2E9a1bYLiKSKwryPaC2Fior4Re/gClT4LzzYPLk8LqyMrwvIpILCvI5\nVlcHM2aEafuam8NsThDWzc1h+4wZqtGLSG5EnbtG2tm1K8zglLRzZ1hPnAipqfzXr+/ZcolI76Ca\nfI7Nnt02yB9wQNs1hPevvbZnyyUivYOCfI7V1sL06W0DfarGRjjrLHj00Z4tl4j0DgryORaPw4oV\ncOGFYV7WVNu3h+1PPRX2ExGJmoJ8jl16KZSXw0EHQUtLWJqa9v580EHh/cuUkFlEckBBPseuvTYE\n8S99Cfr3h7/8Bc49N6z79w+jbMrL4Zpr8l1SESlFCvI5NmwYLF0KDQ1w3XUwfjw89hhMmABz5oQ2\n+aVLw34509oKy5bB/Plh3dqaw4uJSCHREMoeUFMTxsEvXw4DBoSAH4+HNvlRo3ogwE+bBqtWhW+U\nigqorg6FicVyeGERKQTmqYO182D8+PG+evXqvJahpC1bBhdfHL5ZkuJxWLw4DPsRkaJkZi+6+/iu\n9lNzTalbs2bf8ZuNjbB2bX7KIyI9SkG+1I0dG5poUlVUQFVVfsojIj1KQb7U1dSENvh4HMzCuro6\nbBeRkqeO11IXi4VO1tra0ERTVRUCvDpdRXoFBfneIBYLnazqaBXpddRcIyJSwhTkRURKmIK8iEgJ\nU5AXESlhCvIZ0kTcIlKMsg7yZnaAmd1tZm+Y2TYzW2NmJTUIWxNxi0ixiqIm3wd4C5gEHAh8H7jf\nzIZEcO6800TcIlLMsh4n7+6NwA0pm5aZ2UZgHLAp2/PnmybiFpFiFnmbvJkdBgwHXk2zz1VmttrM\nVtfX10ddhEhpIm4RKWaRBnkzKwcWAfe4e6d1W3e/y93Hu/v4Qw89NMoiRE4TcYtIMesyyJvZCjPz\nTpZnUvYrA+4FdgFfz2GZe5Qm4haRYtZlkHf3ye5unSwnApiZAXcDhwEXuHtzjsvdYzQRt4gUs6ia\na34GjATOdvftXe1cTDQRt4gUsyjGyQ8GvgJUAe+bWUNiuSTr0hWAgpiIW0Skm6IYQvkGYBGUpWDl\ndSJuEZEsaCLvYtHaGob6rFkTpvTTxB8ivVqmE3lr0pBi0NoK06bBqlWhfaiiIkzht3y5Ar2IpKUE\nZcWgtjYE+IaG8JhtQ0N4raQ5ItKFXhXkizaT5Jo1+z6N1dgY5mwVEUmj1wT5os4kOXZsaKJJVVER\nJuUWEUmjVwT5os8kWVMT2uDjcTAL6+rqsF1EJI1e0fFa9JkkY7HQyVpbG5poqqo0ukZEMtIravIl\nkUkyFguZ0ubODWsFeBHJQK8I8sokKSK9VdEF+eQImYEDoawsrLsaIaNMkiLSWxVVkE8dIbNtW2hP\n37at6xEyyiQpIr1V0QT59iNkUnU1QqZgMkm2tsKyZTB/fli3tub4giLS2xXN6Jqbbto3uLfX3Ay3\n3AILFrTd3j6T5M03h78CJkyAb38bTj45w0yS2eSPUWoCEcmDoqnJ//rXmQX5e+/t+L2ampAxcseO\nkEmyrKxtJskuh5wng/TFF8P114f1tGmZ18aVmkBE8qBognxDQ/b7DRsWavlbt4bYvHVreJ1RquBs\ng7RSE4hIHhRNkM905EvORshkG6SVmkBE8qBognxyhEw6OR0hk22QVmoCEcmDopk0pK4uDJNsaup8\nn+TImZzM1BRFx2my41apCUQkS5lOGlI0QR5CfJwxI3SwpnbClpeHZenSHFeMFaRFpECUZJCHUKO/\n5ZYwiiY51+pll4Ux7pprVUR6i5IN8iIiknmQj7zj1cw+Y2Y7zOzXUZ9bRET2Ty5G19wJvJCD84qI\nyH6KNMib2UXAFuDxKM8rIiLdE1mQN7OBwDygy6k3zOwqM1ttZqvr6+ujKoKIiLQTZU1+PnC3u7/V\n1Y7ufpe7j3f38YceemiERRARkVQZBXkzW2Fm3snyjJlVAacCt+S2uCIisj8ySjXs7pPTvW9m/woM\nAd40M4A4EDOzUe5+bJZlFBGRbooqn/xdwH0pr2cTgv7XIjq/iIh0QyRB3t2bgD1ZZcysAdjh7upV\nFRHJo5zMDOXuN+TivCIisn+KJtWwiIjsPwV5EZESpiAvIlLCFORFREqYgryISAlTkBcRKWEK8iIi\nJUxBXkSkhCnIi4iUMAV5EZESpiAvIlLCFORFREqYgryISAlTkBcRKWEK8iIiJUxBXkSkhCnIi4iU\nMAV5EZESpiAvIlLCFORFREqYgryISAmLLMib2UVmts7MGs2szsxOiurckWpthWXLYP78sG5tzXeJ\nRERypk8UJzGz04AfAxcCzwOHR3HeyLW2wrRpsGoVNDZCRQVUV8Py5RCL5bt0IiKRi6om/0Ngnrs/\n5+673f0dd38nonNHp7Y2BPiGBnAP61WrwnYRkRKUdZA3sxgwHjjUzF43s7fNbIGZ9UtzzFVmttrM\nVtfX12dbhMytWRNq8KkaG2Ht2p4rg4hID4qiJn8YUA7MAE4CqoCxwNzODnD3u9x9vLuPP/TQQyMo\nQobGjg1NNKkqKqCqqufKICLSg7oM8ma2wsy8k+UZYHti1zvc/T13/xC4GTgzlwXvlpqa0AYfj4NZ\nWFdXh+0iIiWoy45Xd5/c1T5m9jbgURQop2Kx0MlaWxuaaKqqQoBXp6uIlKhIRtcAvwK+YWaPAs3A\nvwLLIjp3tGIxmD49LCIiJS6qID8fOATYAOwA7gd+FNG5RUSkmyIJ8u7eDMxKLCIiUiCU1kBEpIQp\nyIuIlDAFeRGREmbu+R35aGb1wBt5LUTPOQT4MN+FKGC6P+np/nStN92jwe7e5dOkeQ/yvYmZrXb3\n8fkuR6HS/UlP96drukf7UnONiEgJU5AXESlhCvI96658F6DA6f6kp/vTNd2jdtQmLyJSwlSTFxEp\nYQryIiIlTEFeRKSEKchnwcy+npjGcKeZ/WfK9uPN7L/N7CMzqzezJWbW6eTmiYlZdphZQ2L5a498\ngBxLc39GJbZ/nFgeM7NRac5zsJk9YGaNZvaGmX2+Rz5AD4jwHvWq36F2+1yfmMTo1DTnGWJmT5pZ\nk5mtT7dvqVGQz867wL8Bv2y3/ROEXv4hwGBgGyHnfjpfd/d4YvnnqAuaJ53dn3cJ00UeTHhC8SHg\nvjTnuRPYRZhq8hLgZ2Z2dOSlzY+o7hH0rt8hAMxsGOE+vdfFeRYDa4BBwPeApWbWg3OP5o+CfBbc\n/Xfu/iCwud32Wndf4u5/d/cmYAEwMS+FzKM092eLu2/yMLTLgFbgnzo6h5lVABcA33f3Bnd/hhDw\nLstt6XtGFPeolHV2f1IsAK4jVAI6ZGbDgWOB6919u7v/FniZ8HtV8hTke8bJwKtd7PP/mtmHZvYn\nM5vcA2XKOzPbQphk5g7g/+lkt+FAq7tvSNn2ElAqNfm0MrxHSb3qd8jMZgK73P2RLnY9Gvibu29L\n2dZrfoeimhlKOmFmlcAPgHPT7HYd8BqhNnIR8LCZVbl7XQ8UMW/c/aBETf1yOk9SFwe2ttu2FRiQ\ny7IVigzvEfSy3yEzixO+9E7PYPfOfoeOiLpchUg1+Rwys38CaoFvufvTne3n7qvcfZu773T3e4A/\nAWf2VDnzyd0bgYXA/zKzT3awSwMwsN22gYR+jl4hg3vUG3+Hfgjc6+4bM9i3V/8OKcjniJkNBh4D\n5rv7vft5eLIdtrcoA/rTcc1qA9DHzD6Tsm0MXTd/lZp096gjpf47NBX4ppm9b2bvA58C7jez6zrY\n91Xg02aW+tdfr/kdUpDPgpn1MbO+QAyImVnfxLYjgCeAO919YRfnOMjMpqUcewmhDX957j9BbqW5\nP6eZ2Vgzi5nZQOBm4GNgXftzJGqxvwPmmVmFmU0kNH3t7xdnQYriHvXG3yFCkB8NVCWWd4GvEEZi\ntZHoz1kLXJ84/jygEvhtD32M/HJ3Ld1cgBsINabU5Qbg+sTPDalLynHfBWoTPx8KvED403EL8Bxw\nWr4/W47vz0xgfeK+1AOPAJUd3Z/E64OBB4FG4E3g8/n+bIV0j3rj71AH+20CTk15vRBYmPJ6CLAC\n2A78NXXfUl+UoExEpISpuUZEpIQpyIuIlDAFeRGREqYgLyJSwhTkRURKmIK8iEgJU5CXXi2Rh3xG\nvsshkisK8lKSEsE73fKfiV0PBx7OY1FFckoPQ0lJMrN/THk5Hfg5IaAnbXf39pkJRUqOavJSktz9\n/eRCeNS/zbZkgE9trklMEedmdpGZPWVm281sjZlVmtloM1uZmILwGTMbmno9MzvbzF5MTMG30cx+\nZGb/0OMfXKQdBXmRff0Q+DEwlvAF8RvCpB3fA44D+gK3J3c2s2nAIsIsRUcDXyRMSdfVJB8iOacg\nL7Kvm939EXdfD9xECNx3uPuT7v4qIZhPSdn/e8CN7v4rd69z9ycJk3h81cxKOd2vFAHNDCWyr7+k\n/PxBYv1yu20VZtbfwxy+44Dj2uUyLwP6Af9I15NMi+SMgrzIvppTfvY028pS1j8ElnRwrvpoiyay\nfxTkRbL3Z2CEu7+e74KItKcgL5K9ecAyM3sDuB9oIcxadJy7/195LZn0eup4FcmSuy8HziJ0xj6f\nWOYQZrESySs9DCUiUsJUkxcRKWEK8iIiJUxBXkSkhCnIi4iUMAV5EZESpiAvIlLCFORFREqYgryI\nSAn7/wEMCqZrioPicwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aea8fc2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "is_training = False\n",
    "with tf.Session() as sess:\n",
    "    if is_training:\n",
    "        init.run()\n",
    "        for iteration in range(n_iterations):\n",
    "            X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            if iteration % 100 == 0:\n",
    "                mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                print(iteration, \"\\tMSE:\", mse)\n",
    "        save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "    else:\n",
    "        saver.restore(sess, \"/tmp/my_model.ckpt\")\n",
    "        X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "        y_pred = sess.run(outputs, feed_dict={X: X_new})\n",
    "        \n",
    "        plt.title(\"Testing the model\", fontsize=14)\n",
    "        plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "        plt.plot(t_instance[1:], time_series(t_instance[1:]), \"w*\", markersize=10, label=\"target\")\n",
    "        plt.plot(t_instance[1:], y_pred[0,:,0], \"r.\", markersize=10, label=\"prediction\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout with Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate IRNN...\n",
      "Train on 100000 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 34s - loss: 1.3340 - mean_squared_error: 1.3340 - val_loss: 0.1600 - val_mean_squared_error: 0.1600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aeaa2feeb8>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "def ts_next_batch(batch_size, n_steps,resolution = 0.1):\n",
    "    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)\n",
    "    Ts = t0 + np.arange(0., n_steps + 1) * resolution\n",
    "    ys = time_series(Ts)\n",
    "    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "batch_size = 50\n",
    "hidden_units = 100\n",
    "learning_rate = 0.001\n",
    "n_inputs = 1\n",
    "n_outputs = 1\n",
    "n_steps = 20\n",
    "\n",
    "n_layers = 3\n",
    "keep_prob = 0.5\n",
    "\n",
    "print('Evaluate IRNN...')\n",
    "a = Input(shape=(n_steps,n_inputs))\n",
    "b = SimpleRNN(hidden_units,\n",
    "                        kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                        recurrent_initializer=initializers.Identity(),\n",
    "                        activation='relu' ,  return_sequences=True)(a)\n",
    "b = Dropout(keep_prob)(b)\n",
    "for i in range(n_layers-1): \n",
    "    b = SimpleRNN(hidden_units,\n",
    "                        kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                        recurrent_initializer=initializers.Identity(),\n",
    "                        activation='relu' ,  return_sequences=True)(a)\n",
    "    b = Dropout(keep_prob)(b)\n",
    "b = keras.layers.core.Reshape((-1,n_neurons))(b)\n",
    "b = Dense(1,activation=None)(b)\n",
    "b = keras.layers.core.Reshape((n_steps, n_outputs))(b)\n",
    "optimizer = keras.optimizers.Adamax(lr=learning_rate)\n",
    "model = Model(inputs=[a], outputs=[b])\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mean_squared_error'])\n",
    "\n",
    "X_batch, y_batch = ts_next_batch(batch_size*2000, n_steps)\n",
    "x_test, y_test = ts_next_batch(batch_size*2, n_steps)\n",
    "model.fit(X_batch, y_batch,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEeCAYAAABv8mXfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDvv/AWqSEw5o4P4KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aea901beb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "y_pred = model.predict(X_new,verbose=0)\n",
    "plt.title(\"Testing the model\", fontsize=14)\n",
    "plt.plot(t_instance[:-1], time_series(t_instance[:-1]), \"bo\", markersize=10, label=\"instance\")\n",
    "plt.plot(t_instance[1:], time_series(t_instance[1:]), \"w*\", markersize=10, label=\"target\")\n",
    "plt.plot(t_instance[1:], y_pred[0,:,0], \"r.\", markersize=10, label=\"prediction\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "The Long Short-Term Memory (LSTM) cell was proposed in (Hochreiter-Schmidhuber,1997), and it was gradually improved over the years by several researchers. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect long-term dependencies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 0 Train accuracy = 0.966667 Test accuracy = 0.9582\n",
      "Epoch 1 Train accuracy = 0.966667 Test accuracy = 0.9684\n",
      "Epoch 2 Train accuracy = 0.986667 Test accuracy = 0.973\n",
      "Epoch 3 Train accuracy = 0.986667 Test accuracy = 0.9803\n",
      "Epoch 4 Train accuracy = 0.993333 Test accuracy = 0.9848\n",
      "Epoch 5 Train accuracy = 0.973333 Test accuracy = 0.986\n",
      "Epoch 6 Train accuracy = 1.0 Test accuracy = 0.9867\n",
      "Epoch 7 Train accuracy = 0.993333 Test accuracy = 0.9862\n",
      "Epoch 8 Train accuracy = 1.0 Test accuracy = 0.9878\n",
      "Epoch 9 Train accuracy = 0.993333 Test accuracy = 0.9878\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "n_layers = 3 \n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons) for _ in range(n_layers)])\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    "top_layer_h_state = states[-1][1]\n",
    "logits = fully_connected(top_layer_h_state, n_outputs, activation_fn=None, scope=\"softmax\")\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((batch_size, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(\"Epoch\", epoch, \"Train accuracy =\", acc_train, \"Test accuracy =\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Evaluate LSTM...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 374s - loss: 0.6565 - acc: 0.7826 - val_loss: 0.2588 - val_acc: 0.9194\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 370s - loss: 0.1966 - acc: 0.9386 - val_loss: 0.1571 - val_acc: 0.9512\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 369s - loss: 0.1331 - acc: 0.9588 - val_loss: 0.1104 - val_acc: 0.9652\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 371s - loss: 0.1061 - acc: 0.9675 - val_loss: 0.0876 - val_acc: 0.9709\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 375s - loss: 0.0831 - acc: 0.9739 - val_loss: 0.0838 - val_acc: 0.9749\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 356s - loss: 0.0696 - acc: 0.9782 - val_loss: 0.0747 - val_acc: 0.9769\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 379s - loss: 0.0587 - acc: 0.9817 - val_loss: 0.0664 - val_acc: 0.9783\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 341s - loss: 0.0494 - acc: 0.9850 - val_loss: 0.0590 - val_acc: 0.9813\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 338s - loss: 0.0417 - acc: 0.9868 - val_loss: 0.0549 - val_acc: 0.9827\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 347s - loss: 0.0349 - acc: 0.9892 - val_loss: 0.0586 - val_acc: 0.9810\n",
      "LSTM test score: 0.0585878778149\n",
      "LSTM test accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "batch_size = 150\n",
    "num_classes = 10\n",
    "epochs = 10 \n",
    "n_neurons = 150\n",
    "n_layers = 3 \n",
    "learning_rate = 0.001\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('Evaluate LSTM...')\n",
    "a = Input(shape=x_train.shape[1:])\n",
    "\n",
    "b = LSTM(n_neurons,return_sequences=True)(a)\n",
    "for i in range(n_layers-2):\n",
    "    b = LSTM(n_neurons,return_sequences=True)(b)\n",
    "\n",
    "b = LSTM(n_neurons,return_sequences=False)(b)\n",
    "\n",
    "b = Dense(num_classes)(b)\n",
    "b = Activation('softmax')(b)\n",
    "\n",
    "optimizer = keras.optimizers.Adamax(lr=learning_rate)\n",
    "model = Model(inputs=[a], outputs=[b])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('LSTM test score:', scores[0])\n",
    "print('LSTM test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributing layers across devices\n",
    "\n",
    "If you try to create each cell in a different device() block, it will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"): # BAD! This is ignored. \n",
    "    layer1 = tf.contrib.rnn.BasicRNNCell( num_units = n_neurons) \n",
    "    \n",
    "with tf.device(\"/gpu:1\"): # BAD! Ignored again. \n",
    "    layer2 = tf.contrib.rnn.BasicRNNCell( num_units = n_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fails because a BasicRNNCell is a cell factory, not a cell per se; no cells get created when you create the factory, and thus no variables do either. The device block is simply ignored. The cells actually get created later. When you call dynamic_rnn(), it calls the MultiRNNCell, which calls each individual BasicRNNCell, which create the actual cells (including their variables). Unfortunately, none of these classes provide any way to control the devices on which the variables get created. If you try to put the dynamic_rnn() call within a device block, the whole RNN gets pinned to a single device. \n",
    "\n",
    "__The trick is to create your own cell wrapper__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01774996 -0.00240297 -0.06099286 ...,  0.01896067  0.0262568\n",
      "   -0.10385772]\n",
      "  [-0.01140774  0.07944129  0.09548181 ...,  0.17136577  0.07142816\n",
      "    0.02270918]\n",
      "  [ 0.01077356 -0.19770344  0.03775163 ...,  0.16440447 -0.03113544\n",
      "   -0.15833262]\n",
      "  ..., \n",
      "  [ 0.18550645 -0.045463   -0.54991585 ...,  0.06169901 -0.02102067\n",
      "   -0.13856457]\n",
      "  [ 0.00486014  0.19622976  0.08843058 ...,  0.06885004  0.170504\n",
      "   -0.2080622 ]\n",
      "  [ 0.07338118 -0.52152467 -0.42612541 ...,  0.47807157 -0.01505137\n",
      "   -0.56541729]]\n",
      "\n",
      " [[ 0.0101305  -0.04053771 -0.00590748 ...,  0.01823444  0.04402071\n",
      "   -0.0789765 ]\n",
      "  [ 0.04542742  0.03866118  0.08369439 ...,  0.09143315  0.0773553\n",
      "    0.03683787]\n",
      "  [ 0.15722416 -0.07377757  0.06061554 ...,  0.22006992  0.00631647\n",
      "   -0.26226565]\n",
      "  ..., \n",
      "  [-0.39800081  0.3084473  -0.19721624 ...,  0.2082005  -0.04525452\n",
      "   -0.24870294]\n",
      "  [-0.50827909  0.06136992 -0.232437   ...,  0.33199015 -0.07412967\n",
      "   -0.4881908 ]\n",
      "  [-0.28511611  0.12590477 -0.36025527 ...,  0.5273127   0.08071721\n",
      "   -0.18075247]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DeviceCellWrapper(tf.contrib.rnn.RNNCell):\n",
    "  def __init__(self, device, cell):\n",
    "    self._cell = cell\n",
    "    self._device = device\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cell.output_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    with tf.device(self._device):\n",
    "        return self._cell(inputs, state, scope)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 5\n",
    "n_neurons = 100\n",
    "devices = [\"/cpu:0\"]*5\n",
    "n_steps = 20\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs])\n",
    "lstm_cells = [DeviceCellWrapper(device, tf.contrib.rnn.BasicRNNCell(num_units=n_neurons))\n",
    "              for device in devices]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(sess.run(outputs, feed_dict={X: rnd.rand(2, n_steps, n_inputs)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Bidirectional LSTM on the IMDB sentiment classification task on Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 277s - loss: 0.4149 - acc: 0.8074 - val_loss: 0.3521 - val_acc: 0.8450\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 249s - loss: 0.2238 - acc: 0.9139 - val_loss: 0.3632 - val_acc: 0.8469\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 262s - loss: 0.1290 - acc: 0.9532 - val_loss: 0.4287 - val_acc: 0.8361\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 279s - loss: 0.0697 - acc: 0.9765 - val_loss: 0.6076 - val_acc: 0.8346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aed46bcc88>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "del model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM on the IMDB sentiment classification task on Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 182s - loss: 0.4149 - acc: 0.8062 - val_loss: 0.3396 - val_acc: 0.8514\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 169s - loss: 0.2322 - acc: 0.9110 - val_loss: 0.4239 - val_acc: 0.8191\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 171s - loss: 0.1457 - acc: 0.9482 - val_loss: 0.4668 - val_acc: 0.8398\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 168s - loss: 0.1000 - acc: 0.9656 - val_loss: 0.5299 - val_acc: 0.8356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aed2933c50>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "del model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM+FC on the IMDB sentiment classification task on Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 163s - loss: 0.4247 - acc: 0.8021 - val_loss: 0.3404 - val_acc: 0.8504\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 157s - loss: 0.2330 - acc: 0.9117 - val_loss: 0.4124 - val_acc: 0.8258\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 156s - loss: 0.1394 - acc: 0.9501 - val_loss: 0.4462 - val_acc: 0.8374\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 156s - loss: 0.0926 - acc: 0.9672 - val_loss: 0.5852 - val_acc: 0.8339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aed0102c88>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "del model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent convolutional network on the IMDB sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 116s - loss: 0.3830 - acc: 0.8228 - val_loss: 0.3617 - val_acc: 0.8458\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 121s - loss: 0.1969 - acc: 0.9251 - val_loss: 0.3810 - val_acc: 0.8485\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 110s - loss: 0.0965 - acc: 0.9669 - val_loss: 0.4206 - val_acc: 0.8420\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 113s - loss: 0.0444 - acc: 0.9858 - val_loss: 0.5578 - val_acc: 0.8397\n",
      "24990/25000 [============================>.] - ETA: 0sTest score: 0.557802463364\n",
      "Test accuracy: 0.839679995275\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "del model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 4\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional network on the IMDB sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 307s - loss: 0.4115 - acc: 0.7988 - val_loss: 0.2973 - val_acc: 0.8733\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 312s - loss: 0.2450 - acc: 0.9002 - val_loss: 0.2842 - val_acc: 0.8824\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 314s - loss: 0.1767 - acc: 0.9320 - val_loss: 0.2847 - val_acc: 0.8828\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 342s - loss: 0.1240 - acc: 0.9539 - val_loss: 0.3543 - val_acc: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ae821e86d8>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "keras.backend.clear_session()\n",
    "del model\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 4\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB datasets with bi-gram embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Adding 2-gram features\n",
      "Average train sequence length: 476\n",
      "Average test sequence length: 428\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 928s - loss: 0.5806 - acc: 0.7869 - val_loss: 0.4315 - val_acc: 0.8599\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 930s - loss: 0.2768 - acc: 0.9310 - val_loss: 0.2990 - val_acc: 0.8939\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 958s - loss: 0.1363 - acc: 0.9718 - val_loss: 0.2601 - val_acc: 0.9014\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 947s - loss: 0.0733 - acc: 0.9887 - val_loss: 0.2427 - val_acc: 0.9040\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 896s - loss: 0.0411 - acc: 0.9952 - val_loss: 0.2350 - val_acc: 0.9066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aee66cfe80>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "keras.backend.clear_session()\n",
    "del model\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for i in range(len(new_list) - ngram_range + 1):\n",
    "            for ngram_value in range(2, ngram_range + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB datasets with bi-gram embeddings and Convolution1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Adding 2-gram features\n",
      "Average train sequence length: 476\n",
      "Average test sequence length: 428\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 1035s - loss: 0.4344 - acc: 0.7766 - val_loss: 0.2947 - val_acc: 0.8744\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 1020s - loss: 0.1569 - acc: 0.9422 - val_loss: 0.3221 - val_acc: 0.8711\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 1070s - loss: 0.0193 - acc: 0.9945 - val_loss: 0.4046 - val_acc: 0.8686\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 1206s - loss: 0.0015 - acc: 0.9998 - val_loss: 0.4027 - val_acc: 0.8808\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 1223s - loss: 1.5838e-04 - acc: 1.0000 - val_loss: 0.4271 - val_acc: 0.8803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aef9ff7c88>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "keras.backend.clear_session()\n",
    "del model\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for i in range(len(new_list) - ngram_range + 1):\n",
    "            for ngram_value in range(2, ngram_range + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "1. [Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton, _A Simple Way to Initialize Recurrent Networks of Rectified Linear Units_, arxiv:1504.00941v2, cs.NE, 7 Apr 2015](http://arxiv.org/pdf/1504.00941v2.pdf)\n",
    "2. [Sepp Hochreiter, Jürgen Schmidhuber, _Long Short-Term Memory_, Neural Computation, November 15, 1997, Vol. 9, No. 8, Pages: 1735-1780 Posted Online March 13, 2006.](http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735#.WP_u9ojythF)\n",
    "3. [Has¸im Sak, Andrew Senior, Franc¸oise Beaufays, LONG SHORT-TERM MEMORY BASED RECURRENT NEURAL NETWORK ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION, 2014](https://arxiv.org/pdf/1402.1128.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
